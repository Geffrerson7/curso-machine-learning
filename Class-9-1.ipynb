{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKgaNd/wVrbBUatyKn1+s5"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8999968,"sourceType":"datasetVersion","datasetId":5421405}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# MOdulos y datos\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#\nfrom sklearn.model_selection import train_test_split\n#\n\n# Submodulo metrics\nfrom sklearn import metrics\n\n# Modelo Base\nfrom sklearn.linear_model import LogisticRegression\n\n# Submodulo ensemble\nfrom sklearn import ensemble\n\n\n# Datos\ndf = pd.read_csv(\"/kaggle/input/credit-card-fraud-detection-2/creditcard.csv\")\n","metadata":{"id":"mZkPIAuttTRA","executionInfo":{"status":"ok","timestamp":1720919451932,"user_tz":300,"elapsed":5364,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"execution":{"iopub.status.busy":"2024-08-13T03:20:24.696936Z","iopub.execute_input":"2024-08-13T03:20:24.697341Z","iopub.status.idle":"2024-08-13T03:20:32.040471Z","shell.execute_reply.started":"2024-08-13T03:20:24.697308Z","shell.execute_reply":"2024-08-13T03:20:32.039160Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Ya es conocido por nosotros que el dataset tiene un problema de desbalanceo de clases\n# en la variable dependiente Class\n\n# La proporcion de clases 0 y 1 que sea 2 a 1\nClase0 = df.loc[df.Class == 0, ].sample(n = 492)\nClase1 = df.loc[df.Class == 1, ]\n\n# Juntemos estas observaciones en un solo dataframe\ndata = pd.concat([Clase0, Clase1], axis = 0)\ndata.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8lWTQ0Zwa-o","executionInfo":{"status":"ok","timestamp":1720919571139,"user_tz":300,"elapsed":285,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"70fe0e8d-bf0d-4b5e-b3ab-f4a84892f4d1","execution":{"iopub.status.busy":"2024-08-13T03:20:38.898474Z","iopub.execute_input":"2024-08-13T03:20:38.899713Z","iopub.status.idle":"2024-08-13T03:20:38.985924Z","shell.execute_reply.started":"2024-08-13T03:20:38.899674Z","shell.execute_reply":"2024-08-13T03:20:38.984614Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 984 entries, 116136 to 281674\nData columns (total 31 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Time    984 non-null    float64\n 1   V1      984 non-null    float64\n 2   V2      984 non-null    float64\n 3   V3      984 non-null    float64\n 4   V4      984 non-null    float64\n 5   V5      984 non-null    float64\n 6   V6      984 non-null    float64\n 7   V7      984 non-null    float64\n 8   V8      984 non-null    float64\n 9   V9      984 non-null    float64\n 10  V10     984 non-null    float64\n 11  V11     984 non-null    float64\n 12  V12     984 non-null    float64\n 13  V13     984 non-null    float64\n 14  V14     984 non-null    float64\n 15  V15     984 non-null    float64\n 16  V16     984 non-null    float64\n 17  V17     984 non-null    float64\n 18  V18     984 non-null    float64\n 19  V19     984 non-null    float64\n 20  V20     984 non-null    float64\n 21  V21     984 non-null    float64\n 22  V22     984 non-null    float64\n 23  V23     984 non-null    float64\n 24  V24     984 non-null    float64\n 25  V25     984 non-null    float64\n 26  V26     984 non-null    float64\n 27  V27     984 non-null    float64\n 28  V28     984 non-null    float64\n 29  Amount  984 non-null    float64\n 30  Class   984 non-null    int64  \ndtypes: float64(30), int64(1)\nmemory usage: 246.0 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Definamos las variables independientes\nX = data.iloc[:, :-1]\ny = data.Class\n\n# Particionamos los datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n# Empecemos ajustando un modelo base\nmodelBase = LogisticRegression(max_iter = 5000)\nmodelBase.fit(X_train, y_train)\ny_forecast = modelBase.predict(X_test)\n\n# Calculemos un indicador de calidad\naccBase = metrics.accuracy_score(y_test, y_forecast)\n100 - 100*accBase\n\n\n# 6.09137055837563\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eda_mi8Lw-pJ","executionInfo":{"status":"ok","timestamp":1720919580798,"user_tz":300,"elapsed":11,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"f3dcb5d8-b7f8-4acd-dc8b-f6a4d8fd3a8b","execution":{"iopub.status.busy":"2024-08-13T03:20:49.528507Z","iopub.execute_input":"2024-08-13T03:20:49.528969Z","iopub.status.idle":"2024-08-13T03:20:49.578381Z","shell.execute_reply.started":"2024-08-13T03:20:49.528933Z","shell.execute_reply":"2024-08-13T03:20:49.577189Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"8.121827411167516"},"metadata":{}}]},{"cell_type":"code","source":"# LIsta de modelos del submodulo ensenble\ndir(ensemble)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dshQJ6glzU64","executionInfo":{"status":"ok","timestamp":1720919660160,"user_tz":300,"elapsed":10,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"cf386fa8-0e0b-4cd3-c63d-a0cb2d68d289","execution":{"iopub.status.busy":"2024-08-13T03:20:53.512103Z","iopub.execute_input":"2024-08-13T03:20:53.513527Z","iopub.status.idle":"2024-08-13T03:20:53.523372Z","shell.execute_reply.started":"2024-08-13T03:20:53.513479Z","shell.execute_reply":"2024-08-13T03:20:53.522130Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['AdaBoostClassifier',\n 'AdaBoostRegressor',\n 'BaggingClassifier',\n 'BaggingRegressor',\n 'BaseEnsemble',\n 'ExtraTreesClassifier',\n 'ExtraTreesRegressor',\n 'GradientBoostingClassifier',\n 'GradientBoostingRegressor',\n 'HistGradientBoostingClassifier',\n 'HistGradientBoostingRegressor',\n 'IsolationForest',\n 'RandomForestClassifier',\n 'RandomForestRegressor',\n 'RandomTreesEmbedding',\n 'StackingClassifier',\n 'StackingRegressor',\n 'VotingClassifier',\n 'VotingRegressor',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '_bagging',\n '_base',\n '_forest',\n '_gb',\n '_gb_losses',\n '_gradient_boosting',\n '_hist_gradient_boosting',\n '_iforest',\n '_stacking',\n '_voting',\n '_weight_boosting']"},"metadata":{}}]},{"cell_type":"code","source":"# Construyamos una lista con todos los modelos de tipo ensemble\nListaModelos = [ensemble.AdaBoostClassifier(),\n                ensemble.BaggingClassifier(),\n                ensemble.ExtraTreesClassifier(),\n                ensemble.GradientBoostingClassifier(),\n                ensemble.HistGradientBoostingClassifier(),\n                ensemble.RandomForestClassifier(),\n                ]\n# Lista de aaccuracy_score\nListaAcc = []\nfor modelo in ListaModelos:\n  modelo.fit(X_train, y_train)\n  y_forecast = modelo.predict(X_test)\n  Acc_Model = metrics.accuracy_score(y_test, y_forecast)\n  ListaAcc.append(Acc_Model)\n\nListaAcc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIB7ve0Szs9G","executionInfo":{"status":"ok","timestamp":1720920126537,"user_tz":300,"elapsed":3035,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"c031266d-a54c-4056-c2f5-21fd9d3fc013","execution":{"iopub.status.busy":"2024-08-13T03:20:58.772554Z","iopub.execute_input":"2024-08-13T03:20:58.772969Z","iopub.status.idle":"2024-08-13T03:21:01.108013Z","shell.execute_reply.started":"2024-08-13T03:20:58.772938Z","shell.execute_reply":"2024-08-13T03:21:01.106867Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[0.9441624365482234,\n 0.9390862944162437,\n 0.9238578680203046,\n 0.934010152284264,\n 0.9238578680203046,\n 0.934010152284264]"},"metadata":{}}]},{"cell_type":"code","source":"# Documentacion\nhelp(ensemble.AdaBoostClassifier)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5peZvwe2Gn2","executionInfo":{"status":"ok","timestamp":1720920344941,"user_tz":300,"elapsed":346,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"e72ce429-ec07-48ab-b014-3bd9871e85bd","execution":{"iopub.status.busy":"2024-08-13T03:21:07.862481Z","iopub.execute_input":"2024-08-13T03:21:07.862960Z","iopub.status.idle":"2024-08-13T03:21:07.875313Z","shell.execute_reply.started":"2024-08-13T03:21:07.862921Z","shell.execute_reply":"2024-08-13T03:21:07.873868Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Help on class AdaBoostClassifier in module sklearn.ensemble._weight_boosting:\n\nclass AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n |  AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')\n |  \n |  An AdaBoost classifier.\n |  \n |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n |  classifier on the original dataset and then fits additional copies of the\n |  classifier on the same dataset but where the weights of incorrectly\n |  classified instances are adjusted such that subsequent classifiers focus\n |  more on difficult cases.\n |  \n |  This class implements the algorithm known as AdaBoost-SAMME [2].\n |  \n |  Read more in the :ref:`User Guide <adaboost>`.\n |  \n |  .. versionadded:: 0.14\n |  \n |  Parameters\n |  ----------\n |  estimator : object, default=None\n |      The base estimator from which the boosted ensemble is built.\n |      Support for sample weighting is required, as well as proper\n |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n |      initialized with `max_depth=1`.\n |  \n |      .. versionadded:: 1.2\n |         `base_estimator` was renamed to `estimator`.\n |  \n |  n_estimators : int, default=50\n |      The maximum number of estimators at which boosting is terminated.\n |      In case of perfect fit, the learning procedure is stopped early.\n |      Values must be in the range `[1, inf)`.\n |  \n |  learning_rate : float, default=1.0\n |      Weight applied to each classifier at each boosting iteration. A higher\n |      learning rate increases the contribution of each classifier. There is\n |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n |      Values must be in the range `(0.0, inf)`.\n |  \n |  algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n |      ``estimator`` must support calculation of class probabilities.\n |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n |      The SAMME.R algorithm typically converges faster than SAMME,\n |      achieving a lower test error with fewer boosting iterations.\n |  \n |  random_state : int, RandomState instance or None, default=None\n |      Controls the random seed given at each `estimator` at each\n |      boosting iteration.\n |      Thus, it is only used when `estimator` exposes a `random_state`.\n |      Pass an int for reproducible output across multiple function calls.\n |      See :term:`Glossary <random_state>`.\n |  \n |  base_estimator : object, default=None\n |      The base estimator from which the boosted ensemble is built.\n |      Support for sample weighting is required, as well as proper\n |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n |      initialized with `max_depth=1`.\n |  \n |      .. deprecated:: 1.2\n |          `base_estimator` is deprecated and will be removed in 1.4.\n |          Use `estimator` instead.\n |  \n |  Attributes\n |  ----------\n |  estimator_ : estimator\n |      The base estimator from which the ensemble is grown.\n |  \n |      .. versionadded:: 1.2\n |         `base_estimator_` was renamed to `estimator_`.\n |  \n |  base_estimator_ : estimator\n |      The base estimator from which the ensemble is grown.\n |  \n |      .. deprecated:: 1.2\n |          `base_estimator_` is deprecated and will be removed in 1.4.\n |          Use `estimator_` instead.\n |  \n |  estimators_ : list of classifiers\n |      The collection of fitted sub-estimators.\n |  \n |  classes_ : ndarray of shape (n_classes,)\n |      The classes labels.\n |  \n |  n_classes_ : int\n |      The number of classes.\n |  \n |  estimator_weights_ : ndarray of floats\n |      Weights for each estimator in the boosted ensemble.\n |  \n |  estimator_errors_ : ndarray of floats\n |      Classification error for each estimator in the boosted\n |      ensemble.\n |  \n |  feature_importances_ : ndarray of shape (n_features,)\n |      The impurity-based feature importances if supported by the\n |      ``estimator`` (when based on decision trees).\n |  \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |  \n |  n_features_in_ : int\n |      Number of features seen during :term:`fit`.\n |  \n |      .. versionadded:: 0.24\n |  \n |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n |      Names of features seen during :term:`fit`. Defined only when `X`\n |      has feature names that are all strings.\n |  \n |      .. versionadded:: 1.0\n |  \n |  See Also\n |  --------\n |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n |      regressor on the original dataset and then fits additional copies of\n |      the regressor on the same dataset but where the weights of instances\n |      are adjusted according to the error of the current prediction.\n |  \n |  GradientBoostingClassifier : GB builds an additive model in a forward\n |      stage-wise fashion. Regression trees are fit on the negative gradient\n |      of the binomial or multinomial deviance loss function. Binary\n |      classification is a special case where only a single regression tree is\n |      induced.\n |  \n |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n |      method used for classification.\n |      Creates a model that predicts the value of a target variable by\n |      learning simple decision rules inferred from the data features.\n |  \n |  References\n |  ----------\n |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n |         on-Line Learning and an Application to Boosting\", 1995.\n |  \n |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.ensemble import AdaBoostClassifier\n |  >>> from sklearn.datasets import make_classification\n |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n |  ...                            n_informative=2, n_redundant=0,\n |  ...                            random_state=0, shuffle=False)\n |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n |  >>> clf.fit(X, y)\n |  AdaBoostClassifier(n_estimators=100, random_state=0)\n |  >>> clf.predict([[0, 0, 0, 0]])\n |  array([1])\n |  >>> clf.score(X, y)\n |  0.983...\n |  \n |  Method resolution order:\n |      AdaBoostClassifier\n |      sklearn.base.ClassifierMixin\n |      BaseWeightBoosting\n |      sklearn.ensemble._base.BaseEnsemble\n |      sklearn.base.MetaEstimatorMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  decision_function(self, X)\n |      Compute the decision function of ``X``.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Returns\n |      -------\n |      score : ndarray of shape of (n_samples, k)\n |          The decision function of the input samples. The order of\n |          outputs is the same of that of the :term:`classes_` attribute.\n |          Binary classification is a special cases with ``k == 1``,\n |          otherwise ``k==n_classes``. For binary classification,\n |          values closer to -1 or 1 mean more like the first or second\n |          class in ``classes_``, respectively.\n |  \n |  predict(self, X)\n |      Predict classes for X.\n |      \n |      The predicted class of an input sample is computed as the weighted mean\n |      prediction of the classifiers in the ensemble.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Returns\n |      -------\n |      y : ndarray of shape (n_samples,)\n |          The predicted classes.\n |  \n |  predict_log_proba(self, X)\n |      Predict class log-probabilities for X.\n |      \n |      The predicted class log-probabilities of an input sample is computed as\n |      the weighted mean predicted class log-probabilities of the classifiers\n |      in the ensemble.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes)\n |          The class probabilities of the input samples. The order of\n |          outputs is the same of that of the :term:`classes_` attribute.\n |  \n |  predict_proba(self, X)\n |      Predict class probabilities for X.\n |      \n |      The predicted class probabilities of an input sample is computed as\n |      the weighted mean predicted class probabilities of the classifiers\n |      in the ensemble.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes)\n |          The class probabilities of the input samples. The order of\n |          outputs is the same of that of the :term:`classes_` attribute.\n |  \n |  staged_decision_function(self, X)\n |      Compute decision function of ``X`` for each boosting iteration.\n |      \n |      This method allows monitoring (i.e. determine error on testing set)\n |      after each boosting iteration.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Yields\n |      ------\n |      score : generator of ndarray of shape (n_samples, k)\n |          The decision function of the input samples. The order of\n |          outputs is the same of that of the :term:`classes_` attribute.\n |          Binary classification is a special cases with ``k == 1``,\n |          otherwise ``k==n_classes``. For binary classification,\n |          values closer to -1 or 1 mean more like the first or second\n |          class in ``classes_``, respectively.\n |  \n |  staged_predict(self, X)\n |      Return staged predictions for X.\n |      \n |      The predicted class of an input sample is computed as the weighted mean\n |      prediction of the classifiers in the ensemble.\n |      \n |      This generator method yields the ensemble prediction after each\n |      iteration of boosting and therefore allows monitoring, such as to\n |      determine the prediction on a test set after each boost.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          The input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Yields\n |      ------\n |      y : generator of ndarray of shape (n_samples,)\n |          The predicted classes.\n |  \n |  staged_predict_proba(self, X)\n |      Predict class probabilities for X.\n |      \n |      The predicted class probabilities of an input sample is computed as\n |      the weighted mean predicted class probabilities of the classifiers\n |      in the ensemble.\n |      \n |      This generator method yields the ensemble predicted class probabilities\n |      after each iteration of boosting and therefore allows monitoring, such\n |      as to determine the predicted class probabilities on a test set after\n |      each boost.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      Yields\n |      ------\n |      p : generator of ndarray of shape (n_samples,)\n |          The class probabilities of the input samples. The order of\n |          outputs is the same of that of the :term:`classes_` attribute.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseWeightBoosting:\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Build a boosted classifier/regressor from the training set (X, y).\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      y : array-like of shape (n_samples,)\n |          The target values.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights. If None, the sample weights are initialized to\n |          1 / n_samples.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Fitted estimator.\n |  \n |  staged_score(self, X, y, sample_weight=None)\n |      Return staged scores for X, y.\n |      \n |      This generator method yields the ensemble score after each iteration of\n |      boosting and therefore allows monitoring, such as to determine the\n |      score on a test set after each boost.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n |      \n |      y : array-like of shape (n_samples,)\n |          Labels for X.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Yields\n |      ------\n |      z : float\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from BaseWeightBoosting:\n |  \n |  feature_importances_\n |      The impurity-based feature importances.\n |      \n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance.\n |      \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : ndarray of shape (n_features,)\n |          The feature importances.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  __getitem__(self, index)\n |      Return the index'th estimator in the ensemble.\n |  \n |  __iter__(self)\n |      Return iterator over estimators in the ensemble.\n |  \n |  __len__(self)\n |      Return the number of estimators in the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  base_estimator_\n |      Estimator used to grow the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Documentacion\nhelp(ensemble.ExtraTreesClassifier)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iGf2I-W2wPG","executionInfo":{"status":"ok","timestamp":1720920514814,"user_tz":300,"elapsed":358,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"adf2cd1a-7ef0-4e50-8acd-4c900ccbb517","execution":{"iopub.status.busy":"2024-08-13T03:21:16.950736Z","iopub.execute_input":"2024-08-13T03:21:16.951694Z","iopub.status.idle":"2024-08-13T03:21:16.961016Z","shell.execute_reply.started":"2024-08-13T03:21:16.951652Z","shell.execute_reply":"2024-08-13T03:21:16.959919Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Help on class ExtraTreesClassifier in module sklearn.ensemble._forest:\n\nclass ExtraTreesClassifier(ForestClassifier)\n |  ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n |  \n |  An extra-trees classifier.\n |  \n |  This class implements a meta estimator that fits a number of\n |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n |  of the dataset and uses averaging to improve the predictive accuracy\n |  and control over-fitting.\n |  \n |  Read more in the :ref:`User Guide <forest>`.\n |  \n |  Parameters\n |  ----------\n |  n_estimators : int, default=100\n |      The number of trees in the forest.\n |  \n |      .. versionchanged:: 0.22\n |         The default value of ``n_estimators`` changed from 10 to 100\n |         in 0.22.\n |  \n |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n |      The function to measure the quality of a split. Supported criteria are\n |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n |      Note: This parameter is tree-specific.\n |  \n |  max_depth : int, default=None\n |      The maximum depth of the tree. If None, then nodes are expanded until\n |      all leaves are pure or until all leaves contain less than\n |      min_samples_split samples.\n |  \n |  min_samples_split : int or float, default=2\n |      The minimum number of samples required to split an internal node:\n |  \n |      - If int, then consider `min_samples_split` as the minimum number.\n |      - If float, then `min_samples_split` is a fraction and\n |        `ceil(min_samples_split * n_samples)` are the minimum\n |        number of samples for each split.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_samples_leaf : int or float, default=1\n |      The minimum number of samples required to be at a leaf node.\n |      A split point at any depth will only be considered if it leaves at\n |      least ``min_samples_leaf`` training samples in each of the left and\n |      right branches.  This may have the effect of smoothing the model,\n |      especially in regression.\n |  \n |      - If int, then consider `min_samples_leaf` as the minimum number.\n |      - If float, then `min_samples_leaf` is a fraction and\n |        `ceil(min_samples_leaf * n_samples)` are the minimum\n |        number of samples for each node.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_weight_fraction_leaf : float, default=0.0\n |      The minimum weighted fraction of the sum total of weights (of all\n |      the input samples) required to be at a leaf node. Samples have\n |      equal weight when sample_weight is not provided.\n |  \n |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n |      The number of features to consider when looking for the best split:\n |  \n |      - If int, then consider `max_features` features at each split.\n |      - If float, then `max_features` is a fraction and\n |        `max(1, int(max_features * n_features_in_))` features are considered at each\n |        split.\n |      - If \"auto\", then `max_features=sqrt(n_features)`.\n |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n |      - If \"log2\", then `max_features=log2(n_features)`.\n |      - If None, then `max_features=n_features`.\n |  \n |      .. versionchanged:: 1.1\n |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n |  \n |      .. deprecated:: 1.1\n |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n |          in 1.3.\n |  \n |      Note: the search for a split does not stop until at least one\n |      valid partition of the node samples is found, even if it requires to\n |      effectively inspect more than ``max_features`` features.\n |  \n |  max_leaf_nodes : int, default=None\n |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n |      Best nodes are defined as relative reduction in impurity.\n |      If None then unlimited number of leaf nodes.\n |  \n |  min_impurity_decrease : float, default=0.0\n |      A node will be split if this split induces a decrease of the impurity\n |      greater than or equal to this value.\n |  \n |      The weighted impurity decrease equation is the following::\n |  \n |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n |                              - N_t_L / N_t * left_impurity)\n |  \n |      where ``N`` is the total number of samples, ``N_t`` is the number of\n |      samples at the current node, ``N_t_L`` is the number of samples in the\n |      left child, and ``N_t_R`` is the number of samples in the right child.\n |  \n |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n |      if ``sample_weight`` is passed.\n |  \n |      .. versionadded:: 0.19\n |  \n |  bootstrap : bool, default=False\n |      Whether bootstrap samples are used when building trees. If False, the\n |      whole dataset is used to build each tree.\n |  \n |  oob_score : bool, default=False\n |      Whether to use out-of-bag samples to estimate the generalization score.\n |      Only available if bootstrap=True.\n |  \n |  n_jobs : int, default=None\n |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n |      context. ``-1`` means using all processors. See :term:`Glossary\n |      <n_jobs>` for more details.\n |  \n |  random_state : int, RandomState instance or None, default=None\n |      Controls 3 sources of randomness:\n |  \n |      - the bootstrapping of the samples used when building trees\n |        (if ``bootstrap=True``)\n |      - the sampling of the features to consider when looking for the best\n |        split at each node (if ``max_features < n_features``)\n |      - the draw of the splits for each of the `max_features`\n |  \n |      See :term:`Glossary <random_state>` for details.\n |  \n |  verbose : int, default=0\n |      Controls the verbosity when fitting and predicting.\n |  \n |  warm_start : bool, default=False\n |      When set to ``True``, reuse the solution of the previous call to fit\n |      and add more estimators to the ensemble, otherwise, just fit a whole\n |      new forest. See :term:`Glossary <warm_start>` and\n |      :ref:`gradient_boosting_warm_start` for details.\n |  \n |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n |      Weights associated with classes in the form ``{class_label: weight}``.\n |      If not given, all classes are supposed to have weight one. For\n |      multi-output problems, a list of dicts can be provided in the same\n |      order as the columns of y.\n |  \n |      Note that for multioutput (including multilabel) weights should be\n |      defined for each class of every column in its own dict. For example,\n |      for four-class multilabel classification weights should be\n |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n |      [{1:1}, {2:5}, {3:1}, {4:1}].\n |  \n |      The \"balanced\" mode uses the values of y to automatically adjust\n |      weights inversely proportional to class frequencies in the input data\n |      as ``n_samples / (n_classes * np.bincount(y))``\n |  \n |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n |      weights are computed based on the bootstrap sample for every tree\n |      grown.\n |  \n |      For multi-output, the weights of each column of y will be multiplied.\n |  \n |      Note that these weights will be multiplied with sample_weight (passed\n |      through the fit method) if sample_weight is specified.\n |  \n |  ccp_alpha : non-negative float, default=0.0\n |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n |      subtree with the largest cost complexity that is smaller than\n |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n |      :ref:`minimal_cost_complexity_pruning` for details.\n |  \n |      .. versionadded:: 0.22\n |  \n |  max_samples : int or float, default=None\n |      If bootstrap is True, the number of samples to draw from X\n |      to train each base estimator.\n |  \n |      - If None (default), then draw `X.shape[0]` samples.\n |      - If int, then draw `max_samples` samples.\n |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n |        `max_samples` should be in the interval `(0.0, 1.0]`.\n |  \n |      .. versionadded:: 0.22\n |  \n |  Attributes\n |  ----------\n |  estimator_ : :class:`~sklearn.tree.ExtraTreesClassifier`\n |      The child estimator template used to create the collection of fitted\n |      sub-estimators.\n |  \n |      .. versionadded:: 1.2\n |         `base_estimator_` was renamed to `estimator_`.\n |  \n |  base_estimator_ : ExtraTreesClassifier\n |      The child estimator template used to create the collection of fitted\n |      sub-estimators.\n |  \n |      .. deprecated:: 1.2\n |          `base_estimator_` is deprecated and will be removed in 1.4.\n |          Use `estimator_` instead.\n |  \n |  estimators_ : list of DecisionTreeClassifier\n |      The collection of fitted sub-estimators.\n |  \n |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n |      The classes labels (single output problem), or a list of arrays of\n |      class labels (multi-output problem).\n |  \n |  n_classes_ : int or list\n |      The number of classes (single output problem), or a list containing the\n |      number of classes for each output (multi-output problem).\n |  \n |  feature_importances_ : ndarray of shape (n_features,)\n |      The impurity-based feature importances.\n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance.\n |  \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |  \n |  n_features_in_ : int\n |      Number of features seen during :term:`fit`.\n |  \n |      .. versionadded:: 0.24\n |  \n |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n |      Names of features seen during :term:`fit`. Defined only when `X`\n |      has feature names that are all strings.\n |  \n |      .. versionadded:: 1.0\n |  \n |  n_outputs_ : int\n |      The number of outputs when ``fit`` is performed.\n |  \n |  oob_score_ : float\n |      Score of the training dataset obtained using an out-of-bag estimate.\n |      This attribute exists only when ``oob_score`` is True.\n |  \n |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n |      Decision function computed with out-of-bag estimate on the training\n |      set. If n_estimators is small it might be possible that a data point\n |      was never left out during the bootstrap. In this case,\n |      `oob_decision_function_` might contain NaN. This attribute exists\n |      only when ``oob_score`` is True.\n |  \n |  See Also\n |  --------\n |  ExtraTreesRegressor : An extra-trees regressor with random splits.\n |  RandomForestClassifier : A random forest classifier with optimal splits.\n |  RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n |  \n |  Notes\n |  -----\n |  The default values for the parameters controlling the size of the trees\n |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n |  unpruned trees which can potentially be very large on some data sets. To\n |  reduce memory consumption, the complexity and size of the trees should be\n |  controlled by setting those parameter values.\n |  \n |  References\n |  ----------\n |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n |         trees\", Machine Learning, 63(1), 3-42, 2006.\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.ensemble import ExtraTreesClassifier\n |  >>> from sklearn.datasets import make_classification\n |  >>> X, y = make_classification(n_features=4, random_state=0)\n |  >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n |  >>> clf.fit(X, y)\n |  ExtraTreesClassifier(random_state=0)\n |  >>> clf.predict([[0, 0, 0, 0]])\n |  array([1])\n |  \n |  Method resolution order:\n |      ExtraTreesClassifier\n |      ForestClassifier\n |      sklearn.base.ClassifierMixin\n |      BaseForest\n |      sklearn.base.MultiOutputMixin\n |      sklearn.ensemble._base.BaseEnsemble\n |      sklearn.base.MetaEstimatorMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ForestClassifier:\n |  \n |  predict(self, X)\n |      Predict class for X.\n |      \n |      The predicted class of an input sample is a vote by the trees in\n |      the forest, weighted by their probability estimates. That is,\n |      the predicted class is the one with highest mean probability\n |      estimate across the trees.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n |          The predicted classes.\n |  \n |  predict_log_proba(self, X)\n |      Predict class log-probabilities for X.\n |      \n |      The predicted class log-probabilities of an input sample is computed as\n |      the log of the mean predicted class probabilities of the trees in the\n |      forest.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n |          The class probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  predict_proba(self, X)\n |      Predict class probabilities for X.\n |      \n |      The predicted class probabilities of an input sample are computed as\n |      the mean predicted class probabilities of the trees in the forest.\n |      The class probability of a single tree is the fraction of samples of\n |      the same class in a leaf.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n |          The class probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseForest:\n |  \n |  apply(self, X)\n |      Apply trees in the forest to X, return leaf indices.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      X_leaves : ndarray of shape (n_samples, n_estimators)\n |          For each datapoint x in X and for each tree in the forest,\n |          return the index of the leaf x ends up in.\n |  \n |  decision_path(self, X)\n |      Return the decision path in the forest.\n |      \n |      .. versionadded:: 0.18\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      indicator : sparse matrix of shape (n_samples, n_nodes)\n |          Return a node indicator matrix where non zero elements indicates\n |          that the samples goes through the nodes. The matrix is of CSR\n |          format.\n |      \n |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n |          gives the indicator value for the i-th estimator.\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Build a forest of trees from the training set (X, y).\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Internally, its dtype will be converted\n |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csc_matrix``.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          The target values (class labels in classification, real numbers in\n |          regression).\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights. If None, then samples are equally weighted. Splits\n |          that would create child nodes with net zero or negative weight are\n |          ignored while searching for a split in each node. In the case of\n |          classification, splits are also ignored if they would result in any\n |          single class carrying a negative weight in either child node.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Fitted estimator.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from BaseForest:\n |  \n |  feature_importances_\n |      The impurity-based feature importances.\n |      \n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance.\n |      \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : ndarray of shape (n_features,)\n |          The values of this array sum to 1, unless all trees are single node\n |          trees consisting of only the root node, in which case it will be an\n |          array of zeros.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  __getitem__(self, index)\n |      Return the index'th estimator in the ensemble.\n |  \n |  __iter__(self)\n |      Return iterator over estimators in the ensemble.\n |  \n |  __len__(self)\n |      Return the number of estimators in the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  base_estimator_\n |      Estimator used to grow the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Documentacion\nhelp(ensemble.RandomForestClassifier)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdBn8RPq25o9","executionInfo":{"status":"ok","timestamp":1720920563809,"user_tz":300,"elapsed":332,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"7a1f3dac-6dcb-43f8-d391-bfa94433590c","execution":{"iopub.status.busy":"2024-08-13T03:21:25.909455Z","iopub.execute_input":"2024-08-13T03:21:25.910491Z","iopub.status.idle":"2024-08-13T03:21:25.923254Z","shell.execute_reply.started":"2024-08-13T03:21:25.910449Z","shell.execute_reply":"2024-08-13T03:21:25.921920Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n\nclass RandomForestClassifier(ForestClassifier)\n |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n |  \n |  A random forest classifier.\n |  \n |  A random forest is a meta estimator that fits a number of decision tree\n |  classifiers on various sub-samples of the dataset and uses averaging to\n |  improve the predictive accuracy and control over-fitting.\n |  The sub-sample size is controlled with the `max_samples` parameter if\n |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n |  each tree.\n |  \n |  Read more in the :ref:`User Guide <forest>`.\n |  \n |  Parameters\n |  ----------\n |  n_estimators : int, default=100\n |      The number of trees in the forest.\n |  \n |      .. versionchanged:: 0.22\n |         The default value of ``n_estimators`` changed from 10 to 100\n |         in 0.22.\n |  \n |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n |      The function to measure the quality of a split. Supported criteria are\n |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n |      Note: This parameter is tree-specific.\n |  \n |  max_depth : int, default=None\n |      The maximum depth of the tree. If None, then nodes are expanded until\n |      all leaves are pure or until all leaves contain less than\n |      min_samples_split samples.\n |  \n |  min_samples_split : int or float, default=2\n |      The minimum number of samples required to split an internal node:\n |  \n |      - If int, then consider `min_samples_split` as the minimum number.\n |      - If float, then `min_samples_split` is a fraction and\n |        `ceil(min_samples_split * n_samples)` are the minimum\n |        number of samples for each split.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_samples_leaf : int or float, default=1\n |      The minimum number of samples required to be at a leaf node.\n |      A split point at any depth will only be considered if it leaves at\n |      least ``min_samples_leaf`` training samples in each of the left and\n |      right branches.  This may have the effect of smoothing the model,\n |      especially in regression.\n |  \n |      - If int, then consider `min_samples_leaf` as the minimum number.\n |      - If float, then `min_samples_leaf` is a fraction and\n |        `ceil(min_samples_leaf * n_samples)` are the minimum\n |        number of samples for each node.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_weight_fraction_leaf : float, default=0.0\n |      The minimum weighted fraction of the sum total of weights (of all\n |      the input samples) required to be at a leaf node. Samples have\n |      equal weight when sample_weight is not provided.\n |  \n |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n |      The number of features to consider when looking for the best split:\n |  \n |      - If int, then consider `max_features` features at each split.\n |      - If float, then `max_features` is a fraction and\n |        `max(1, int(max_features * n_features_in_))` features are considered at each\n |        split.\n |      - If \"auto\", then `max_features=sqrt(n_features)`.\n |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n |      - If \"log2\", then `max_features=log2(n_features)`.\n |      - If None, then `max_features=n_features`.\n |  \n |      .. versionchanged:: 1.1\n |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n |  \n |      .. deprecated:: 1.1\n |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n |          in 1.3.\n |  \n |      Note: the search for a split does not stop until at least one\n |      valid partition of the node samples is found, even if it requires to\n |      effectively inspect more than ``max_features`` features.\n |  \n |  max_leaf_nodes : int, default=None\n |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n |      Best nodes are defined as relative reduction in impurity.\n |      If None then unlimited number of leaf nodes.\n |  \n |  min_impurity_decrease : float, default=0.0\n |      A node will be split if this split induces a decrease of the impurity\n |      greater than or equal to this value.\n |  \n |      The weighted impurity decrease equation is the following::\n |  \n |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n |                              - N_t_L / N_t * left_impurity)\n |  \n |      where ``N`` is the total number of samples, ``N_t`` is the number of\n |      samples at the current node, ``N_t_L`` is the number of samples in the\n |      left child, and ``N_t_R`` is the number of samples in the right child.\n |  \n |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n |      if ``sample_weight`` is passed.\n |  \n |      .. versionadded:: 0.19\n |  \n |  bootstrap : bool, default=True\n |      Whether bootstrap samples are used when building trees. If False, the\n |      whole dataset is used to build each tree.\n |  \n |  oob_score : bool, default=False\n |      Whether to use out-of-bag samples to estimate the generalization score.\n |      Only available if bootstrap=True.\n |  \n |  n_jobs : int, default=None\n |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n |      context. ``-1`` means using all processors. See :term:`Glossary\n |      <n_jobs>` for more details.\n |  \n |  random_state : int, RandomState instance or None, default=None\n |      Controls both the randomness of the bootstrapping of the samples used\n |      when building trees (if ``bootstrap=True``) and the sampling of the\n |      features to consider when looking for the best split at each node\n |      (if ``max_features < n_features``).\n |      See :term:`Glossary <random_state>` for details.\n |  \n |  verbose : int, default=0\n |      Controls the verbosity when fitting and predicting.\n |  \n |  warm_start : bool, default=False\n |      When set to ``True``, reuse the solution of the previous call to fit\n |      and add more estimators to the ensemble, otherwise, just fit a whole\n |      new forest. See :term:`Glossary <warm_start>` and\n |      :ref:`gradient_boosting_warm_start` for details.\n |  \n |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n |      Weights associated with classes in the form ``{class_label: weight}``.\n |      If not given, all classes are supposed to have weight one. For\n |      multi-output problems, a list of dicts can be provided in the same\n |      order as the columns of y.\n |  \n |      Note that for multioutput (including multilabel) weights should be\n |      defined for each class of every column in its own dict. For example,\n |      for four-class multilabel classification weights should be\n |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n |      [{1:1}, {2:5}, {3:1}, {4:1}].\n |  \n |      The \"balanced\" mode uses the values of y to automatically adjust\n |      weights inversely proportional to class frequencies in the input data\n |      as ``n_samples / (n_classes * np.bincount(y))``\n |  \n |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n |      weights are computed based on the bootstrap sample for every tree\n |      grown.\n |  \n |      For multi-output, the weights of each column of y will be multiplied.\n |  \n |      Note that these weights will be multiplied with sample_weight (passed\n |      through the fit method) if sample_weight is specified.\n |  \n |  ccp_alpha : non-negative float, default=0.0\n |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n |      subtree with the largest cost complexity that is smaller than\n |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n |      :ref:`minimal_cost_complexity_pruning` for details.\n |  \n |      .. versionadded:: 0.22\n |  \n |  max_samples : int or float, default=None\n |      If bootstrap is True, the number of samples to draw from X\n |      to train each base estimator.\n |  \n |      - If None (default), then draw `X.shape[0]` samples.\n |      - If int, then draw `max_samples` samples.\n |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n |        `max_samples` should be in the interval `(0.0, 1.0]`.\n |  \n |      .. versionadded:: 0.22\n |  \n |  Attributes\n |  ----------\n |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n |      The child estimator template used to create the collection of fitted\n |      sub-estimators.\n |  \n |      .. versionadded:: 1.2\n |         `base_estimator_` was renamed to `estimator_`.\n |  \n |  base_estimator_ : DecisionTreeClassifier\n |      The child estimator template used to create the collection of fitted\n |      sub-estimators.\n |  \n |      .. deprecated:: 1.2\n |          `base_estimator_` is deprecated and will be removed in 1.4.\n |          Use `estimator_` instead.\n |  \n |  estimators_ : list of DecisionTreeClassifier\n |      The collection of fitted sub-estimators.\n |  \n |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n |      The classes labels (single output problem), or a list of arrays of\n |      class labels (multi-output problem).\n |  \n |  n_classes_ : int or list\n |      The number of classes (single output problem), or a list containing the\n |      number of classes for each output (multi-output problem).\n |  \n |  n_features_in_ : int\n |      Number of features seen during :term:`fit`.\n |  \n |      .. versionadded:: 0.24\n |  \n |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n |      Names of features seen during :term:`fit`. Defined only when `X`\n |      has feature names that are all strings.\n |  \n |      .. versionadded:: 1.0\n |  \n |  n_outputs_ : int\n |      The number of outputs when ``fit`` is performed.\n |  \n |  feature_importances_ : ndarray of shape (n_features,)\n |      The impurity-based feature importances.\n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance.\n |  \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |  \n |  oob_score_ : float\n |      Score of the training dataset obtained using an out-of-bag estimate.\n |      This attribute exists only when ``oob_score`` is True.\n |  \n |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n |      Decision function computed with out-of-bag estimate on the training\n |      set. If n_estimators is small it might be possible that a data point\n |      was never left out during the bootstrap. In this case,\n |      `oob_decision_function_` might contain NaN. This attribute exists\n |      only when ``oob_score`` is True.\n |  \n |  See Also\n |  --------\n |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n |      tree classifiers.\n |  \n |  Notes\n |  -----\n |  The default values for the parameters controlling the size of the trees\n |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n |  unpruned trees which can potentially be very large on some data sets. To\n |  reduce memory consumption, the complexity and size of the trees should be\n |  controlled by setting those parameter values.\n |  \n |  The features are always randomly permuted at each split. Therefore,\n |  the best found split may vary, even with the same training data,\n |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n |  of the criterion is identical for several splits enumerated during the\n |  search of the best split. To obtain a deterministic behaviour during\n |  fitting, ``random_state`` has to be fixed.\n |  \n |  References\n |  ----------\n |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.ensemble import RandomForestClassifier\n |  >>> from sklearn.datasets import make_classification\n |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n |  ...                            n_informative=2, n_redundant=0,\n |  ...                            random_state=0, shuffle=False)\n |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n |  >>> clf.fit(X, y)\n |  RandomForestClassifier(...)\n |  >>> print(clf.predict([[0, 0, 0, 0]]))\n |  [1]\n |  \n |  Method resolution order:\n |      RandomForestClassifier\n |      ForestClassifier\n |      sklearn.base.ClassifierMixin\n |      BaseForest\n |      sklearn.base.MultiOutputMixin\n |      sklearn.ensemble._base.BaseEnsemble\n |      sklearn.base.MetaEstimatorMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ForestClassifier:\n |  \n |  predict(self, X)\n |      Predict class for X.\n |      \n |      The predicted class of an input sample is a vote by the trees in\n |      the forest, weighted by their probability estimates. That is,\n |      the predicted class is the one with highest mean probability\n |      estimate across the trees.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n |          The predicted classes.\n |  \n |  predict_log_proba(self, X)\n |      Predict class log-probabilities for X.\n |      \n |      The predicted class log-probabilities of an input sample is computed as\n |      the log of the mean predicted class probabilities of the trees in the\n |      forest.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n |          The class probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  predict_proba(self, X)\n |      Predict class probabilities for X.\n |      \n |      The predicted class probabilities of an input sample are computed as\n |      the mean predicted class probabilities of the trees in the forest.\n |      The class probability of a single tree is the fraction of samples of\n |      the same class in a leaf.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n |          The class probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseForest:\n |  \n |  apply(self, X)\n |      Apply trees in the forest to X, return leaf indices.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      X_leaves : ndarray of shape (n_samples, n_estimators)\n |          For each datapoint x in X and for each tree in the forest,\n |          return the index of the leaf x ends up in.\n |  \n |  decision_path(self, X)\n |      Return the decision path in the forest.\n |      \n |      .. versionadded:: 0.18\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, its dtype will be converted to\n |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      indicator : sparse matrix of shape (n_samples, n_nodes)\n |          Return a node indicator matrix where non zero elements indicates\n |          that the samples goes through the nodes. The matrix is of CSR\n |          format.\n |      \n |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n |          gives the indicator value for the i-th estimator.\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Build a forest of trees from the training set (X, y).\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Internally, its dtype will be converted\n |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n |          converted into a sparse ``csc_matrix``.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          The target values (class labels in classification, real numbers in\n |          regression).\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights. If None, then samples are equally weighted. Splits\n |          that would create child nodes with net zero or negative weight are\n |          ignored while searching for a split in each node. In the case of\n |          classification, splits are also ignored if they would result in any\n |          single class carrying a negative weight in either child node.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Fitted estimator.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from BaseForest:\n |  \n |  feature_importances_\n |      The impurity-based feature importances.\n |      \n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance.\n |      \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : ndarray of shape (n_features,)\n |          The values of this array sum to 1, unless all trees are single node\n |          trees consisting of only the root node, in which case it will be an\n |          array of zeros.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  __getitem__(self, index)\n |      Return the index'th estimator in the ensemble.\n |  \n |  __iter__(self)\n |      Return iterator over estimators in the ensemble.\n |  \n |  __len__(self)\n |      Return the number of estimators in the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n |  \n |  base_estimator_\n |      Estimator used to grow the ensemble.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n","output_type":"stream"}]}]}